# -*- coding: utf-8 -*-
"""hashing_crypt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uwPyVFHEhmiwTlP-GSux9IIyGZakaySk
"""

import hashlib
import csv
import time
import numpy as np

# Define the hash functions and sample data
hash_functions = {
    "MD5": hashlib.md5,
    "SHA-1": hashlib.sha1,
    "SHA-256": hashlib.sha256,
    "SHA-512": hashlib.sha512,
    "SHA3-256": hashlib.sha3_256,
    "Blake2b": lambda: hashlib.blake2b(key=b"sample_key")
}

datasets = [
    b"Hello, world!",
    b"The quick brown fox jumps over the lazy dog",
    b"OpenAI creates and trains AI models.",
    b"Data integrity is crucial in information security.",
    b"Cryptographic hash functions are widely used."
]

# Helper function to compute the number of differing bits
def differing_bits(hash1, hash2):
    diff = 0
    for c1, c2 in zip(hash1, hash2):
        diff += bin(c1 ^ c2).count('1')
    return diff

# Helper function to compute entropy
def compute_entropy(data):
    data_len = len(data)
    if data_len == 0:
        return 0
    probabilities = [float(data.count(c)) / data_len for c in set(data)]
    entropy = - sum([p * np.log2(p) for p in probabilities])
    return entropy

# Initialize the results list
results = []

# Process each dataset with each hash function
for data in datasets:
    for name, func in hash_functions.items():
        # Initialize hash function
        hasher = func()

        # Measure Compression Ratio
        input_size = len(data) * 8  # Input size in bits
        hasher.update(data)
        hash_value = hasher.digest()
        hash_value_hex = hasher.hexdigest()
        output_size = len(hash_value) * 8  # Output size in bits
        compression_ratio = input_size / output_size

        # Measure Speed/Throughput
        start_time = time.time()
        for _ in range(1000):
            hasher = func()
            hasher.update(data)
            hasher.digest()
        end_time = time.time()
        throughput = 1000 / (end_time - start_time)  # Hashes per second

        # Measure Avalanche Effect and Sensitivity
        changed_bit = bytes([data[0] ^ 0b00000001]) + data[1:]
        hasher = func()
        hasher.update(changed_bit)
        changed_hash_value = hasher.digest()
        differing = differing_bits(hash_value, changed_hash_value)
        total_bits = len(hash_value) * 8
        avalanche_effect = differing / total_bits * 100  # Percentage of changed bits

        # Measure Entropy
        entropy = compute_entropy(hash_value)

        # Measure Memory Usage (approximation)
        memory_usage = output_size / 8  # Output size in bytes

        # Other properties (theoretical or not directly measurable)
        hash_value_length = len(hash_value) * 8
        security_level = "High" if hash_value_length >= 256 else "Moderate"
        parallelizability = "Moderate"  # General observation; depends on specific implementation
        resistance_length_extension = "No" if name in ["MD5", "SHA-1", "SHA-256"] else "Yes"
        energy_consumption = "Unknown"  # Requires specific hardware measurement
        side_channel_resistance = "Unknown"  # Requires specific testing conditions
        efficiency = (end_time - start_time) / (len(data) * 1000)  # Time per byte hashed (approximation)

        # Append results
        results.append({
            "Hash Function": name,
            "Input Data": data.decode('utf-8', errors='ignore'),
            "Hash Value": hash_value_hex,
            "Compression Ratio": compression_ratio,
            "Throughput (Hashes/sec)": throughput,
            "Avalanche Effect (%)": avalanche_effect,
            "Sensitivity (Bits Changed)": differing,
            "Entropy": entropy,
            "Memory Usage (bytes)": memory_usage,
            "Hash Value Length (bits)": hash_value_length,
            "Security Level": security_level,
            "Parallelizability": parallelizability,
            "Resistance to Length Extension": resistance_length_extension,
            "Energy Consumption": energy_consumption,
            "Side-channel Resistance": side_channel_resistance,
            "Efficiency (Time per Byte)": efficiency
        })

# Define the CSV file paths
csv_file_paths = ["hash_functions_properties_part1.csv", "hash_functions_properties_part2.csv"]

# Split the results into two parts if necessary
split_index = len(results) // 2 if len(results) > 10 else len(results)
parts = [results[:split_index], results[split_index:]]

# Save the results in CSV files
for i, part in enumerate(parts):
    with open(csv_file_paths[i], mode='w', newline='') as file:
        writer = csv.DictWriter(file, fieldnames=[
            "Hash Function", "Input Data", "Hash Value", "Compression Ratio", "Throughput (Hashes/sec)",
            "Avalanche Effect (%)", "Sensitivity (Bits Changed)", "Entropy", "Memory Usage (bytes)",
            "Hash Value Length (bits)", "Security Level", "Parallelizability",
            "Resistance to Length Extension", "Energy Consumption", "Side-channel Resistance",
            "Efficiency (Time per Byte)"
        ])
        writer.writeheader()
        for result in part:
            writer.writerow(result)

print(f"Hash functions properties have been saved to {csv_file_paths[0]} and {csv_file_paths[1]}")